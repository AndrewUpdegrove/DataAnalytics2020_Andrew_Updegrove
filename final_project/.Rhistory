upside_down <- matrix(sapply(grayscale[i,],as.numeric), nrow=w)
orig <- flipdim(upside_down, dim=2)
mirror <- flipdim(orig, dim=1)
sym <- sqrt(sum((mirror-orig)^2))
feat_data <- rbind(feat_data,c(avg_inten,sym))
}
colnames(feat_data) <- c("Intensity","Symmetry")
feat_data$digit = digits
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = digits)) + geom_point()
head(feat_data)
feat_data <- data.frame(Intensity=numeric(), Symmetry=numeric())
for(i in 1:nrow(grayscale)) {
avg_inten <- sum(grayscale[i,])/length(grayscale[i,])
upside_down <- matrix(sapply(grayscale[i,],as.numeric), nrow=w)
orig <- flipdim(upside_down, dim=2)
mirror <- flipdim(orig, dim=1)
sym <- sum(abs(mirror-orig))/length(grayscale[i,])
feat_data <- rbind(feat_data,c(avg_inten,sym))
}
colnames(feat_data) <- c("Intensity","Symmetry")
feat_data$digit = digits
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = digits)) + geom_point()
#------------------------------------------
# Visualization
#------------------------------------------
x <- c(0:w)
y <- c(0:w)
d1 <- grayscale[1,]
d1 <- matrix(sapply(d1,as.numeric), nrow=w)
image(x,y,flipdim(d1,dim=2), col = gray.colors(256,start=0,end=1))
d2 <- grayscale[2,]
d2 <- matrix(sapply(d2,as.numeric), nrow=16)
test_1 <- flipdim(d2,dim=2)
image(x,y,test_1, col = gray.colors(256,start=0,end=1))
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = digits)) + geom_point()
a = matrix(1:4,nrow=2)
a[,1:2]
a = matrix(1:16,nrow = 4)
a
a[,1:2]
feat_data <- data.frame(Intensity=numeric(), Symmetry=numeric())
for(i in 1:nrow(grayscale)) {
avg_inten <- sum(grayscale[i,])/length(grayscale[i,])
upside_down <- matrix(sapply(grayscale[i,],as.numeric), nrow=w)
orig <- flipdim(upside_down, dim=2)
mirror <- flipdim(orig, dim=1)
sym <- sum(abs(mirror[,1:8]-orig[,1:8]))/length(grayscale[i,])
feat_data <- rbind(feat_data,c(avg_inten,sym))
}
colnames(feat_data) <- c("Intensity","Symmetry")
feat_data$digit = digits
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = digits)) + geom_point()
help("legend")
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = digits, shape = digits)) + geom_point() + legen
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = digits, shape = digits)) + geom_point()
digits = as.integer(data[,1])
grayscale = data[,2:ncol(data)]
grayscale <- (1-grayscale)*.5
w = floor(sqrt(dim(grayscale)[2]))
feat_data <- data.frame(Intensity=numeric(), Symmetry=numeric())
for(i in 1:nrow(grayscale)) {
avg_inten <- sum(grayscale[i,])/length(grayscale[i,])
upside_down <- matrix(sapply(grayscale[i,],as.numeric), nrow=w)
orig <- flipdim(upside_down, dim=2)
mirror <- flipdim(orig, dim=1)
sym <- sum(abs(mirror[,1:8]-orig[,1:8]))/length(grayscale[i,])
feat_data <- rbind(feat_data,c(avg_inten,sym))
}
colnames(feat_data) <- c("Intensity","Symmetry")
feat_data$digit = digits
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = digits, shape = digits)) + geom_point()
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = digits)) + geom_point()
feat_data <- data.frame(Intensity=numeric(), Symmetry=numeric())
for(i in 1:nrow(grayscale)) {
avg_inten <- sum(grayscale[i,])/length(grayscale[i,])
upside_down <- matrix(sapply(grayscale[i,],as.numeric), nrow=w)
orig <- flipdim(upside_down, dim=2)
mirror <- flipdim(orig, dim=1)
sym <- sqrt(sum((mirror-orig)^2))
feat_data <- rbind(feat_data,c(avg_inten,sym))
}
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = ifelse(x == 1, 'blue','red'))) + geom_point()
colnames(feat_data) <- c("Intensity","Symmetry")
feat_data$digit = digits
ggplot(feat_data, aes(x=Intensity,y=Symmetry, colour = ifelse(x == 1, 'blue','red'))) + geom_point()
ggplot(feat_data, aes(x=Intensity,y=Symmetry, col = ifelse(x == 1, 'blue','red'))) + geom_point()
aes(x=Intensity,y=Symmetry, col = ifelse(x == 1, 'blue','red'))
ggplot(feat_data, aes(x=Intensity,y=Symmetry, color= digit == 1)) + geom_point() + scale_color_manual(name = 'Digit', values = setNames(c('red','blue'),c(T,F)))
ggplot(feat_data, aes(x=Intensity,y=Symmetry, color= digit == 1)) + geom_point() + scale_color_manual(name = 'Digit', values = setNames(c('red','blue'),c(1,5)))
feat_data <- data.frame(Intensity=numeric(), Symmetry=numeric())
for(i in 1:nrow(grayscale)) {
avg_inten <- sum(grayscale[i,])/length(grayscale[i,])
upside_down <- matrix(sapply(grayscale[i,],as.numeric), nrow=w)
orig <- flipdim(upside_down, dim=2)
mirror <- flipdim(orig, dim=1)
sym <- sqrt(sum((mirror-orig)^2))
feat_data <- rbind(feat_data,c(avg_inten,sym))
}
colnames(feat_data) <- c("Intensity","Symmetry")
feat_data$digit = digits
ggplot(feat_data, aes(x=Intensity,y=Symmetry, color= digit == 1)) + geom_point() + scale_color_manual(name = 'Digit', values = setNames(c('red','blue'),c(1,5)))
ggplot(feat_data, aes(x=Intensity,y=Symmetry, color= digit == 1)) + geom_point() + scale_color_manual(name = 'Digit', values = setNames(c('red','blue'),c(T,F)))
help("scale_color_manual")
ggplot(feat_data, aes(x=Intensity,y=Symmetry, color= digit == 1)) + geom_point() + scale_color_manual(name = 'Digit',labels=c('1','5'), values = setNames(c('red','blue'),c(T,F)))
help("scale_shape_manual")
ggplot(feat_data, aes(x=Intensity,y=Symmetry, color= digit == 1, shape = digit == 1)) +
geom_point() +
scale_color_manual(name = 'Digit',labels=c('1','5'), values = setNames(c('blue','red'),c(T,F))) +
scale_shape_manual(name = NULL, values = setNames(c(1,4),c(T,F)))
help("scale_shape_manual")
ggplot(feat_data, aes(x=Intensity,y=Symmetry, color= digit == 1, shape = digit == 1)) +
geom_point() +
scale_color_manual(name = 'Digit',labels=c('1','5'), values = setNames(c('blue','red'),c(T,F))) +
scale_shape_manual(name = NULL, values = setNames(c(1,4),c(T,F)), guide= FALSE)
library(ggplot2)
library(pracma)
raw_data = read.table(file.choose(), header = FALSE, sep = " ", dec= ".")
View(raw_data)
data("Titanic")
require(rpart)
require(rpart.plot)
require(ggplot2)
help(hclust)
help(ctree)
??ctree
require(tree)
str(Titanic)
head(Titanic)
names(Titantic)
name(Titantic)
colname(Titantic)
colnames(Titantic)
titan.data = as.data.frame(Titanic)
colnames(titan.data)
titan.data[1,]
help("geom_histogram")
help("geom_boxplot")
ggplot(data = titan.data, aes(x = class) + geom_bar()
d
ggplot(data = titan.data, aes(x = class)) + geom_bar()
ggplot(data = titan.data, aes(x = Class)) + geom_bar()
data(Titanic)
size(Titanic)
dim(Titanic)
ggplot(data = titan.data, aes(x = Class, y = Freq)) + geom_histogram()
ggplot(data = titan.data, aes(x = Class, y = Freq)) + geom_bar()
barplot(prop.table(table(titan.data)))
table(titan.data)
titan.data[1,]
titan.data[2,]
titan.data[3,]
titan.data[4,]
titan.data[5,]
titan.data[10,]
titan.data[11,]
titan.data[14,]
titan.data[125,]
titan.data[25,]
titan.data[26,]
titan.data[27,]
titan.data[28,]
sum(titan.data[,5])
help("rpart")
rpart(Survived ~ ., data=titan.data)
titan.part <- rpart(Survived ~ ., data=titan.data)
plot(titan.part)
text(titan.part)
titan.tree <- ctree(Survived ~ ., data = tita.data)
require(party)
titan.tree <- ctree(Survived ~ ., data = tita.data)
titan.tree <- ctree(Survived ~ ., data = titan.data)
plot(titan.tree)
install.packages("titanic")
data("titanic")
require(titanic)
data("Titanic")
force(Titanic)
knitr::kable(head(titanic_train))
titanic_train
data(Titanic)
titan.part <- rpart(Survived ~ ., data= Titanic)
rpart.plot(titan.part)
library(ISLR)
library(MASS)
library(boot)
set.seed(1)
help("sample")
#create learning regression model using training data
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
head(Auto)
summary(Auto)
nrow(Auto)
train = sample(Auto,196)
train = sample(392,196)
#create learning regression model using training data
lm.fit <- lm(mpg~horsepower, data = Auto, subset = train)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
help(mpg-predict)
help("mpg-predit")
??mpg-predict
attach(Auto)
mean((mpg-predict(lm.fit,Auto))[-train]^2)
#create quadratic regression model using training data
lm.fit2 <- lm(mpg~poly(horsepower,2), data = Auto, subset = train)
mean((mpg-predict(lm.fit2,Auto))[-train]^2)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
#create cubic regression model using training data
lm.fit3 <- lm(mpg~poly(horsepower, 3), data = Auto, subset = train)
mean((mpg-predict(lm.fit3,Auto))[-train]^2)
set.seed(17)
cv.error.10 = rep(0,10)
for(i in 1:10) {
glm.fit = glm(mpg ~ ploy(horsepower, i), data = Auto)
cv.error.10[i] = cv.glm(Auto,glm.fit, K=10)$delta[1]
}
for(i in 1:10) {
glm.fit = glm(mpg ~ poly(horsepower, i), data = Auto)
cv.error.10[i] = cv.glm(Auto,glm.fit, K=10)$delta[1]
}
cv.error.10
install.packages("randomForest")
#--------------------------------------------------------------
# Random Forest
#--------------------------------------------------------------
library(randomForest)
#load car.data dataset
data1 <- read.csv(file.choose(), header = TRUE)
colname(data1) <- c("BuyingPrice", "Maintenance", "NumDoors", "NumPersons", "BootSpace", "Safety", "Condition")
colnames(data1) <- c("BuyingPrice", "Maintenance", "NumDoors", "NumPersons", "BootSpace", "Safety", "Condition")
head(data1)
levels(data1$Condition)
train <- sample(nrow(data1, .7*nrow(data1), replace = FALSE))
train <- sample(nrow(data1), .7*nrow(data1), replace = FALSE))
train <- sample(nrow(data1), .7*nrow(data1), replace = FALSE)
ValidSet <- data1[-train,]
help("randomForest")
model1 <- randomForest(Condition ~ ., data = TrainSet, importance = TRUE)
TrainSet <- data1[train,]
model1 <- randomForest(Condition ~ ., data = TrainSet, importance = TRUE)
summary(TrainSet)
summary(ValidSet)
levels(data1$Condition)
View(data1)
View(data1)
help("levels")
as.factor(data1)
FactorData <- as.factor(data1)
levels(FactorData$Condition)
head(data1)
str(data1)
levels(data1$Condition)
train <- sample(nrow(data1), 0.7*nrow(data1), replace = FALSE)
TrainSet <- data1[train,]
ValidSet <- data1[-train,]
summary(TrainSet)
summary(ValidSet)
model1 <- randomForest(Condition ~ ., data = TrainSet, importance = TRUE)
lapply(data1, levels)
lapply(data1, function(i) levels(factor(i)))
data1[] <- lapply(data1, factor)
head(data1)
head(data1)
str(data1)
lapply(data1, function(i) levels(factor(i)))
levels(data1$Condition)
set.seed(100)
train <- sample(nrow(data1), 0.7*nrow(data1), replace = FALSE)
TrainSet <- data1[train,]
ValidSet <- data1[-train,]
summary(TrainSet)
summary(ValidSet)
model1 <- randomForest(Condition ~ ., data = TrainSet, importance = TRUE)
model1
model2 <- randomForest(Condition ~ ., data= TrainSet, ntree = 500, mtry = 6, importance = TRUE)
model2
predTrain <- predict(model2, TrainSet, type = "class")
table(predTrain, TrainSet$Condition)
predValid <- predict(model2, ValidSet, type = "class")
table(predValid, ValidSet$Condition)
#We can also use importance() function to check important variables
# The below function show the drop in mean accuracy for each of the variables
# to check the important variables
importance(model2)
varImpPlot(model2)
a = c()
i = 5
for (i in 3:8) {
model3 <- randomForest(Condition ~ ., data = TrainSet, ntree=500, mtry = i, importance = TRUE)
predValid <- predict(model3, ValidSet, type = "class")
a[i-2] = mean(predValid == ValidSet$Condition)
}
for (i in 3:8) {
model3 <- randomForest(Condition ~ ., data = TrainSet, ntree=500, mtry = i, importance = TRUE)
predValid <- predict(model3, ValidSet, type = "class")
a[i-2] = mean(predValid == ValidSet$Condition)
}
a
plot(3:8, a)
#comparing forest to decision tree
library(rpart)
install.packages("caret")
install.packages("e1071")
library(caret)
library(e1071)
model_dt <- train(Condition ~ ., data= TrainSet, method = "rpart")
model_dt_1 = predict(model_dt, data = TrainSet)
table(model_dt_1, TrainSet$Condition)
mean(model_dt_1 == TrainSet$Condition)
table(model_dt_1, TrainSet$Condition)
mean(model_dt_1 == TrainSet$Condition)
model_dt_vs = predict(model_dt, newdata = ValidSet)
table(model_dt_vs, ValidSet$Condition)
table(model_dt_vs == ValidSet$Condition)
W = matrix(c(1,2,3))
W
L = diag(3)
L
W.t
W.T
t(W)
t(W) %*% L
t(W) %*% L
t(W) %*% W
L = matrix(c(1,1,1,1,1,1,1,1,1), 3)
L
W.t %*% L
t(W) %*% L
t(W) %*% L %*% L
t(W) %*% L %*% L %*% W
L = matrix(rep(1/2,9), 3)
L
t(W) %*% L %*% L %*% W
sqrt(108)
108/36
L = matrix(rep(1/3, 9),3)
t(W) %*% L %*% L %*% W
L = matrix(rep(1,9),3)
t(W) %*% L %*% L %*% W
t(W) %*% L
t(W) %*% L %*% L
18*6
L = matrix(c(0,1,1,1,0,1,1,1,0),3)
L
t(W) %*% L
t(W) %*% L %*% L
t(W) %*% L %*% L %*% W
L = matrix(c(0,0,1,0,1,0,1,0,0),3)
L
t(W) %*% L %*% L %*% W
t(W) %*% L
t(W) %*% L %*% L
t(W) %*% L %*% L %*% W
L = matrix(c(1,1,1,0,0,0,0,0,0),3)
L
L = t(L)
L
t(W) %*% t(L)
t(W) %*% t(L) %*% L
t(W) %*% t(L) %*% L %*% W
L = matrix(c(1,1,1,0,0,0,0,0,0),3)
L
t(W) %*% t(L) %*% L %*% W
L = matrix(c(1,1,1,1,0,0,1,0,0),3)
L
t(W) %*% t(L) %*% L %*% W
L = matrix(c(1,1,1,1,0,0,0,0,0,0,0,0,0,0,0,0),4)
L
L = t(L)
W = matrix(c(1,2,3,4))
W
W = matrix(c(4,9,2,3))
W
t(W) %*% t(L) %*% L %*% W
sqrt(324)
require(rpart)
Swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_rpart) # try some different plot options
swiss_rpart <- rpart(Fertility ~ Agriculture + Education + Catholic, data = swiss)
plot(swiss_rpart) # try some different plot options
text(swiss_rpart) # try some different text options
# Regression Tree Example
# build the  tree
fitM <- rpart(Mileage~Price + Country + Reliability + Type, method="anova", data=cu.summary)
printcp(fitM) # display the results
plotcp(fitM)
summary(fitM)
par(mfrow=c(1,2))
rsq.rpart(fitM) # visualize cross-validation results
# plot tree
plot(fitM, uniform=TRUE, main="Regression Tree for Mileage ")
text(fitM, use.n=TRUE, all=TRUE, cex=.8)
# prune the tree
pfitM<- prune(fitM, cp=0.01160389) # from cptable??? adjust this to see the effect
# plot the pruned tree
plot(pfitM, uniform=TRUE, main="Pruned Regression Tree for Mileage")
text(pfitM, use.n=TRUE, all=TRUE, cex=.8)
post(pfitM, file = "ptree2.ps", title = "Pruned Regression Tree for Mileage")
library(e1071)
data(Glass, package="mlbench")
install.packages("mlbench")
data(Glass, package="mlbench")
index <- 1:nrow(Glass)
testindex <- sample(index, trunc(length(index)/3))
testset <- Glass[testindex,]
trainset <- Glass[-testindex,]
rpart.model <- rpart(Type ~ ., data = trainset)
rpart.pred <- predict(rpart.model, testset[,-10], type = "class")
printcp(rpart.model)
plotcp(rpart.model)
rsq.rpart(rpart.model)
print(rpart.model)
plot(rpart.model,compress=TRUE)
text(rpart.model, use.n=TRUE)
plot(rpart.pred)
fitK <- rpart(Kyphosis ~ Age + Number + Start, method="class", data=kyphosis)
printcp(fitK) # display the results
plotcp(fitK) # visualize cross-validation results
summary(fitK) # detailed summary of splits
# plot tree
plot(fitK, uniform=TRUE, main="Classification Tree for Kyphosis")
text(fitK, use.n=TRUE, all=TRUE, cex=.8)
# create attractive postscript plot of tree
post(fitK, file = "kyphosistree.ps", title = "Classification Tree for Kyphosis") # might need to convert to PDF (distill)
pfitK<- prune(fitK, cp=   fitK$cptable[which.min(fitK$cptable[,"xerror"]),"CP"])
plot(pfitK, uniform=TRUE, main="Pruned Classification Tree for Kyphosis")
require(randomForest)
fitKF <- randomForest(Kyphosis ~ Age + Number + Start,   data=kyphosis)
??kypohsis
??kyphosis
fitKF <- randomForest(Kyphosis ~ Age + Number + Start,   data=kyphosis)
head(kyphosis)
print(fitKF) 	# view results
importance(fitKF) # importance of each predictor
#
fitSwiss <- randomForest(Fertility ~ Agriculture + Education + Catholic, data = swiss)
print(fitSwiss) # view results
importance(fitSwiss) # importance of each predictor
varImpPlot(fitSwiss)
plot(fitSwiss)
varImpPlot(fitSwiss)
plot(fitSwiss)
getTree(fitSwiss,1, labelVar=TRUE)
data(Titanic)
data(Titanic)
require(rpart)
require(ctree)
require(tree)
require(hclust)
help(hclust)
require(randomForest)
titan.part <- rpart(Survived ~ ., data = Titanic)
titanic
head(Titanic)
plot(titan.part)
text(titan.part)
install.packages("titanic")
data(titanic)
??Titanic
help(titanic)
help(Titanic)
data(Titanic)
head(Titanic)
part <- rpart(Survived ~ ., data = Titanic)
plot(part)
text(part)
part <- rpart(Survived ~ ., data = titanic)
data(titanic)
require(titanic)
data(titanic)
part <- rpart(Survived ~ ., data = titanic)
head(titanic_train)
titan.data <- titanic_train
titan.data$PassengerId <- NULL
titan.data$Name <- NULL
part <- rpart(Survived ~ Pclass + Sex + Age + SibSp, data = titanic_train)
plot(part)
text(part)
wine_data <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.data", sep = ",")
View(wine_data)
View(wine_data)
help(wine_data)
wine_names <- read.table("http://archive.ics.uci.edu/ml/machine-learning-databases/wine/wine.names", sep = ",")
View(wine_data)
View(wine_data)
names(wine_dat) <- c("Class", "Alcohol", "Malic acid", "Ash", "Alcalinity of ash", "Magnesium", "Total phenols", "Flavanoids", "Nonflavanoid phenols", "Proanthocyanins", "Color intensity", "Hue", "OD280/OD315 of diluted wines", "Proline")
names(wine_data) <- c("Class", "Alcohol", "Malic acid", "Ash", "Alcalinity of ash", "Magnesium", "Total phenols", "Flavanoids", "Nonflavanoid phenols", "Proanthocyanins", "Color intensity", "Hue", "OD280/OD315 of diluted wines", "Proline")
View(wine_data)
View(wine_data)
names(wine_data) <- c("Class", "Alcohol", "Malic_Acid", "Ash", "Alcalinity_of_Ash", "Magnesium", "Total_Phenols", "Flavanoids", "Nonflavanoid_Phenols", "Proanthocyanins", "Color_Intensity", "Hue", "OD280/OD315_of_Diluted_Wines", "Proline")
#See correlation of values using heatmap
heatmap(cor(wine_data), Rowv = NA, Colv = NA)
help(factor)
cultivar_classes <- factor(wine_data$Class)
cultivar_classes
help("prcomp")
wine_data_PCA <- prcomp(scale(wine_data[,=1]))
wine_data_PCA <- prcomp(scale(wine_data[,-1]))
scale(wine_data[,-1]))
scale(wine_data[,-1])
help("scale")
summary(wine_data_PCA)
(tiff)
require(tiff)
require(rgdal)
require(raster)
setwd(choose.dir())
source("helper_functions_final.R")
file_location <- "CMS_Pantropical_Forest_Biomass_1337/data/AGLB_Deforested_Tropical_Asia_2000.tif"
mangrove.file_location <-"CMS_Mangrove_Cover_1670/data/mangrove_cover_2000-2016_CanGio.tif"
mangrove.raster_image <- raster(mangrove.file_location)
plot(mangrove.raster_image)
